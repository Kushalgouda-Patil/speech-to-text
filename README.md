# ğŸ™ï¸ Voice Assistant â€” Speech-to-Text Microservice

FastAPI microservice that transcribes audio files to text using
[faster-whisper](https://github.com/SYSTRAN/faster-whisper) (a 4Ã— faster
CTranslate2-based implementation of OpenAI Whisper).

Part of the **AI Voice Assistant** microservices suite.

---

## âœ¨ Features

| Feature | Details |
|---|---|
| **Whisper models** | `tiny`, `base`, `small`, `medium`, `large`, `large-v2`, `large-v3` |
| **Audio formats** | WAV, MP3, M4A, OGG, FLAC, WebM, MP4 |
| **Upload modes** | Multipart file upload **or** base64-encoded JSON body |
| **Language** | Auto-detect or force via `language` parameter |
| **Timestamps** | Per-segment start/end times + confidence scores |
| **VAD filtering** | Automatically skips silent regions |
| **Non-blocking** | Transcription runs in a thread-pool, event loop stays free |
| **Observability** | Structured logging, `/health` endpoint |
| **Docker ready** | `Dockerfile` + `docker-compose.yml` included |

---

## ğŸ—‚ï¸ Project Structure

```
voice-assistant-fastapi/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ main.py                 # FastAPI app factory & lifespan
â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â”œâ”€â”€ deps.py             # FastAPI DI helpers
â”‚   â”‚   â”œâ”€â”€ health.py           # GET /health, GET /models
â”‚   â”‚   â””â”€â”€ transcribe.py       # POST /api/v1/transcribe/
â”‚   â”‚                           # POST /api/v1/transcribe/base64
â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â”œâ”€â”€ config.py           # Settings (Pydantic Settings)
â”‚   â”‚   â””â”€â”€ logging.py          # Logging setup
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â””â”€â”€ transcription.py    # Pydantic request/response schemas
â”‚   â””â”€â”€ services/
â”‚       â””â”€â”€ whisper_service.py  # Whisper model wrapper
â”œâ”€â”€ tests/
â”‚   â””â”€â”€ test_transcription.py
â”œâ”€â”€ .env.example
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ Makefile
â””â”€â”€ requirements.txt
```

---

## ğŸš€ Quick Start (Local)

### 1. Create a virtual environment

```bash
python3 -m venv .venv
source .venv/bin/activate      # Windows: .venv\Scripts\activate
```

### 2. Install dependencies

```bash
pip install -r requirements.txt
# macOS â€“ also install ffmpeg (required for audio decoding):
brew install ffmpeg
```

> **Apple Silicon (M1/M2/M3)** â€” set `WHISPER_DEVICE=mps` in `.env` for GPU acceleration.

### 3. Configure environment

```bash
cp .env.example .env
# Edit .env as needed (model, device, etc.)
```

### 4. Run the server

```bash
# Production
uvicorn app.main:app --host 0.0.0.0 --port 8000

# Development (auto-reload)
make dev
```

The service boots, downloads the selected Whisper model on first run, and is ready at **http://localhost:8000**.

- Swagger UI â†’ http://localhost:8000/docs  
- ReDoc â†’ http://localhost:8000/redoc  
- Health â†’ http://localhost:8000/health

---

## ğŸ³ Docker

```bash
# Build & start
make docker-up

# Tear down
make docker-down
```

Model weights are persisted in a named Docker volume (`whisper_cache`) so they
are not re-downloaded on restart.

---

## ğŸ“¡ API Reference

### `POST /api/v1/transcribe/` â€” Upload audio file

**Request** â€” `multipart/form-data`

| Field | Type | Required | Description |
|---|---|---|---|
| `audio` | `File` | âœ… | Audio file (WAV/MP3/M4A/OGG/FLAC/WebM/MP4) |
| `language` | `string` | âŒ | BCP-47 code (`en`, `fr`, `hi` â€¦). Omit for auto-detect. |

**cURL example**

```bash
curl -X POST http://localhost:8000/api/v1/transcribe/ \
     -F "audio=@recording.wav" \
     -F "language=en"
```

**Response** `200 OK`

```json
{
  "text": "Hello, how can I help you today?",
  "language": "en",
  "duration": 3.52,
  "model": "base",
  "segments": [
    {
      "id": 0,
      "start": 0.0,
      "end": 3.52,
      "text": "Hello, how can I help you today?",
      "avg_logprob": -0.312,
      "no_speech_prob": 0.004
    }
  ]
}
```

---

### `POST /api/v1/transcribe/base64` â€” Base64 JSON payload

Useful for callers that cannot send multipart (IoT devices, WebSocket bridges, etc.).

```bash
curl -X POST http://localhost:8000/api/v1/transcribe/base64 \
     -H "Content-Type: application/json" \
     -d '{
       "audio_base64": "<base64-encoded bytes>",
       "filename": "recording.wav",
       "language": "en"
     }'
```

---

### `GET /health`

```json
{ "status": "ok", "model_loaded": true, "whisper_model": "base", "version": "1.0.0" }
```

### `GET /models`

Returns the list of all supported Whisper model variants and descriptions.

---

## âš™ï¸ Configuration

All settings are controlled via environment variables (or `.env` file).

| Variable | Default | Description |
|---|---|---|
| `WHISPER_MODEL` | `base` | Model variant to load |
| `WHISPER_DEVICE` | `cpu` | `cpu`, `cuda`, or `mps` |
| `WHISPER_COMPUTE_TYPE` | `int8` | `int8`, `float16`, `float32` |
| `WHISPER_LANGUAGE` | _(auto)_ | Force a language globally |
| `MAX_UPLOAD_SIZE_MB` | `25` | Max audio file size |
| `LOG_LEVEL` | `INFO` | Logging verbosity |
| `ALLOWED_ORIGINS` | `["*"]` | CORS allowed origins |

---

## ğŸ§ª Tests

```bash
pip install -r requirements-dev.txt
make test
```

---

## ğŸ”® Integration with other microservices

This service is designed to be called by other services in the AI Voice
Assistant pipeline, e.g.:

```
Microphone / Client
      â”‚  audio bytes
      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STT Microservice    â”‚  â† you are here
â”‚  (this repo)         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚  transcribed text (JSON)
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  LLM / NLU Service   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚  response text
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  TTS Microservice    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```
